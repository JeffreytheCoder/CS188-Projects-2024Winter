---
layout: post
comments: true
title: Image Generation
author: Team 13
date: 2024-03-15
---


> In the realm of image generation, deep learning technologies have revolutionized traditional methods, enabling advanced applications like text-to-image generation, style transfer, and image translation with unprecedented effectiveness. This blog highlights the transformative capabilities of deep learning for generating high-quality images. We will compare and contrast Generative Adversarial Networks (GANs) and diffusion models and show the strengths and applications of each model

<!--more-->
{: class="table-of-content"}
* TOC
{:toc}

## Introduction
Image generation has emerged as one of the most fascinating and fast-developing areas in the field of computer vision and deep learning. With tasks ranging from creating fake images that are similar to inputs, to translating images to match a particular artistic style, the possibilities are expansive. These methods not only showcase the creative potential of AI but also highlight the intricate understanding these models have of content and style, foregrounding the nuanced interplay between them. This versatility and depth of understanding make deep learning an indispensable tool in the ongoing innovation within the realm of image generation.

![Examples of AI Generated Images]({{ '/assets/images/team13/intro.png' | relative_url }})
{: style="width: 100%; max-width: 100%;"}
*Fig 1. Examples of AI Generated Images (from Latent Diffusion Model)* [1].


Specifically, we will discuss how deep learning is used for three image generation tasks:

1). **Generating fake images that are similar to a set of inputs** - The goal of generating images that are similar to a given input set is to produce new, unique images that maintain the essence of the original dataset. Deep learning models, particularly Generative Adversarial Networks (GANs), have shown exceptional prowess in this domain. They learn to replicate the complex distribution of the input images, enabling the generation of new images that, while novel, are visually indistinguishable from the originals.

![Examples of face image generation]({{ '/assets/images/team13/dcgan_face.png' | relative_url }})
{: style="width: 100%; max-width: 100%;"}
*Fig 2. Examples of face images generated by Deep Convolutional GAN (DCGAN)* [2].



2). **Neural Style Transfer** - Moving into the artistic domain, neural style transfer represents a particularly mesmerizing application of AI in image generation. Here, deep learning models—often a combination of Convolutional Neural Networks (CNNs) and GANs—are trained to dissect and understand the distinguishing features of an artist's style and then apply it to any given photograph. The result is a harmonious blend that retains the structure of the original photo while adopting the artistic flair of the chosen style. This is not merely a pixel-level transformation but a deep structural reimagining of the image that reflects the unique artistic signatures of color, brushwork, and texture.

![Examples of neural style transfer]({{ '/assets/images/team13/neural_style_transfer_intro.png' | relative_url }})
{: style="width: 100%; max-width: 100%;"}
*Fig 3. Examples of neural style transfer from CNN. Image A is input image and others are transformed copies of the input based on various art styles* [3].

3). **Text-to-image generation** - Text-to-image generation stands out as a testament to the remarkable advancements in natural language processing and computer vision as models must be capable of both understanding the text inputs and generate a coherent image. . Models designed for this task take descriptive text as input and output images that accurately capture the described scenes or objects. Diffusion models work particularly well in this domain.

![Examples of text-to-image generation]({{ '/assets/images/team13/text_image_controlnet.png' | relative_url }})
{: style="width: 100%; max-width: 100%;"}
*Fig 4. Examples of text-to-image generation from ControlNet* [4].


In the remaining of this blog, we will discuss three deep learning models (one for each task) in detail and how they are able to accomplish the image generation tasks. Lastly, we will present the strength and weakness of each architecture and illustrate the potential trade-offs when choosing which architecture to use.

## Fake Celebrity Image Generation with DCGAN
In this section, we will discuss Generative Adversarial Networks (GANs) and show how to create a Deep Convolutional GAN (DCGAN) to generate fake celebrity face images using the CelebA dataset.


### GAN Architecture
GAN consists of two neural networks against each other in a game-theoretic scenario. The first network, known as the *Generator*, receives random input and is tasked with creating images that look as real as possible. The second network, called the *Discriminator*, evaluates an input image to determine whether it is real (from the dataset) or fake (produced by the Generator). Through iterative training, the Generator learns to produce increasingly realistic images, while the Discriminator becomes better at distinguishing real images from fakes.

![GAN Architecture]({{ '/assets/images/team13/gan.png' | relative_url }})
{: style="width: 100%; max-width: 100%;"}
*Fig 5. GAN Architecture* [5].

Specifically, we define $$x$$ be an input image, $$p_{\text{data}}$$ be the probability $$x$$ is from image space, $$D(x)$$ be the discriminator output for $$x$$ (scalar probability that $$x$$ comes from dataset), $$z$$ be a latent space vector sampled from a standard normal distribution, $$p_z(z)$$ be the probability that $$z$$ is from latent vector space, and $$G(z)$$ be the generator output that maps $$z$$ to image space. $$G$$ wants to maximize the value of $$D(G(z))$$, which is the probability that the generator output is classified as comes from dataset (real), whereas $$G$$ wants to maximize $$D(x)$$, which is the probability that a real image is classified as real, and minimize $$D(G(z))$$. Hence, GAN loss function becomes:

$$
\min_{G} \max_{D} V(D, G) = \mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]
$$

The first term $$\mathbb{E}_{x \sim p_{\text{data}}(x)}[\log D(x)]$$ represents the expected log-probability of the discriminator being correct on real data, while the second term $$\mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))]$$ represents the expected log-probability of the discriminator being correct on fake data generated by $$G$$.

### DCGAN Architecture
DCGAN is an CNN-extension of GAN. DCGAN uses convolutional and convolutional-transpose layers in the discriminator and generator respectively. The generator architecture presented in DCGAN paper is shown below:

![DCGAN Generator Architecture]({{ '/assets/images/team13/dcgan_generator.png' | relative_url }})
{: style="width: 100%; max-width: 100%;"}
*Fig 6. DCGAN Generator Architecture* [2].

### Implementation
Here, we will implement DCGAN and generate fake facial images using the CelebA dataset following [Pytorch official tutorial](https://pytorch.org/tutorials/beginner/dcgan_faces_tutorial.html). We will include some core code snippets for model architectures/weight initializations/etc. Fully implementation can be found in the tutorial.

First, let us visualize some example images from CelebA dataset:

![CelebA Training Images]({{ '/assets/images/team13/celeb_train.png' | relative_url }})
{: style="width: 100%; max-width: 100%;"}
*Fig 7. CelebA Training Images*.

**Weight Initialization**

In the DCGAN paper, the author specified that all model weights should be randomly initialized from a normal distribution with mean = 0 and std = 0.2. We therefore define the `weights_init` method as follows:

```
# custom weights initialization called on netG and netD
def weights_init(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        nn.init.normal_(m.weight.data, 0.0, 0.02)
    elif classname.find('BatchNorm') != -1:
        nn.init.normal_(m.weight.data, 1.0, 0.02)
        nn.init.constant_(m.bias.data, 0)
```

**Generator**

As previously shown, the generator will map the latent space vector $$z$$ to image space. This is done through a series of transpoed 2d convolutions. Also, DCGAN innovatively adds batch norm layers after transposed convolutions, which help with the flow of gradients during training. Here nz is the length of the z input vector (in DCGAN case 100), ngf is the size of the feature maps that are propagated through the generator (in DCGAN case 64), and nc is the number of channels in the output image (set to 3 for RGB images)
```
# Generator Code

class Generator(nn.Module):
    def __init__(self, ngpu):
        super(Generator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is Z, going into a convolution
            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),
            nn.BatchNorm2d(ngf * 8),
            nn.ReLU(True),
            # state size. (ngf*8) x 4 x 4
            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 4),
            nn.ReLU(True),
            # state size. (ngf*4) x 8 x 8
            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf * 2),
            nn.ReLU(True),
            # state size. (ngf*2) x 16 x 16
            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ngf),
            nn.ReLU(True),
            # state size. (ngf) x 32 x 32
            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),
            nn.Tanh()
            # state size. (nc) x 64 x 64
        )

    def forward(self, input):
        return self.main(input)

# Create the generator
netG = Generator(ngpu).to(device)

# Handle multi-gpu if desired
if (device.type == 'cuda') and (ngpu > 1):
    netG = nn.DataParallel(netG, list(range(ngpu)))

# Apply the weights_init function to randomly initialize all weights
#  to mean=0, stdev=0.2.
netG.apply(weights_init)
```

**Discriminator**

As mentioned, the discriminator is a binary classification network that takes an image as input and outputs a scalar probability that the input image is real (as opposed to fake). In our case, the discriminator takes in a 3 * 64 * 64 image. The DCGAN paper mentions it is a good practice to use strided convolution rather than pooling to downsample because it lets the network learn its own pooling function. Also batch norm and leaky relu functions promote healthy gradient flow which is critical for the learning process of both $$G$$ and $$D$$.

```
class Discriminator(nn.Module):
    def __init__(self, ngpu):
        super(Discriminator, self).__init__()
        self.ngpu = ngpu
        self.main = nn.Sequential(
            # input is (nc) x 64 x 64
            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf) x 32 x 32
            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 2),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*2) x 16 x 16
            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 4),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*4) x 8 x 8
            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),
            nn.BatchNorm2d(ndf * 8),
            nn.LeakyReLU(0.2, inplace=True),
            # state size. (ndf*8) x 4 x 4
            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),
            nn.Sigmoid()
        )

    def forward(self, input):
        return self.main(input)

# Create the Discriminator
netD = Discriminator(ngpu).to(device)

# Handle multi-gpu if desired
if (device.type == 'cuda') and (ngpu > 1):
    netD = nn.DataParallel(netD, list(range(ngpu)))

# Apply the weights_init function to randomly initialize all weights
#  to mean=0, stdev=0.2.
netD.apply(weights_init)
```

**Loss Functions and Optimizers**

We will use the Binary Cross Entropy loss function to train $$G$$ and $$D$$:

$$
\mathcal{L}(x, y) = \mathbf{L} = \{l_1,\dots,l_N\}^\top, \quad l_n = -[y_n \cdot \log(x_n) + (1 - y_n) \cdot \log(1 - x_n)]
$$

BCE loss provides calculation for both $$log[D(x)]$$ and $$log[1 - D(G(z))]$$ components in the GAN loss function. We also define real labels as 1 and fake labels as 0, which are also the conventions in the original GAN paper. Finally, we will have two separate optimizers for $$D$$ and $$G$$. We follow the DCGAN paper and use Adam optimizers with learning rate 0.0002 and Beta1 = 0.5 for both models.

```
# Initialize BCELoss function
criterion = nn.BCELoss()

# Create batch of latent vectors that we will use to visualize
#  the progression of the generator
fixed_noise = torch.randn(batch_size, nz, 1, 1, device=device)

# Establish convention for real and fake labels during training
real_label = 1
fake_label = 0

# Setup Adam optimizers for both G and D
optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))
optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))
```

**Training**

The training process is divided into two phases to update the Discriminator and the Generator. For the Discriminator, the aim is to enhance its ability to distinguish real from fake inputs by maximizing the sum of the probabilities $$\log(D(x)) + \log(1 - D(G(z)))$$ by computing the loss for real samples (with $$\log(D(x))$$) and fake samples (with $$\log(1 - D(G(z)))$$), then updating the Discriminator's parameters. For the Discriminator, we intend to maximize $$\log(D(G(z)))$$ (rather than minimizing $$\log(1 - D(G(z)))$$) for better gradient flow, using the Discriminator’s output as a measure of quality and updating the Generator's parameters accordingly.

```
# Training Loop

# Lists to keep track of progress
img_list = []
G_losses = []
D_losses = []
iters = 0

print("Starting Training Loop...")
# For each epoch
for epoch in range(num_epochs):
    # For each batch in the dataloader
    for i, data in enumerate(dataloader, 0):

        ############################
        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
        ###########################
        ## Train with all-real batch
        netD.zero_grad()
        # Format batch
        real_cpu = data[0].to(device)
        b_size = real_cpu.size(0)
        label = torch.full((b_size,), real_label, device=device)
        # Forward pass real batch through D
        output = netD(real_cpu).view(-1)
        # Calculate loss on all-real batch
        errD_real = criterion(output.float(), label.float())
        # Calculate gradients for D in backward pass
        errD_real.backward()
        D_x = output.mean().item()

        ## Train with all-fake batch
        # Generate batch of latent vectors
        noise = torch.randn(b_size, nz, 1, 1, device=device)
        # Generate fake image batch with G
        fake = netG(noise)
        label.fill_(fake_label)
        # Classify all fake batch with D
        output = netD(fake.detach()).view(-1)
        # Calculate D's loss on the all-fake batch
        errD_fake = criterion(output.float(), label.float())
        # Calculate the gradients for this batch
        errD_fake.backward()
        D_G_z1 = output.mean().item()
        # Add the gradients from the all-real and all-fake batches
        errD = errD_real + errD_fake
        # Update D
        optimizerD.step()

        ############################
        # (2) Update G network: maximize log(D(G(z)))
        ###########################
        netG.zero_grad()
        label.fill_(real_label)  # fake labels are real for generator cost
        # Since we just updated D, perform another forward pass of all-fake batch through D
        output = netD(fake).view(-1)
        # Calculate G's loss based on this output
        errG = criterion(output.float(), label.float())
        # Calculate gradients for G
        errG.backward()
        D_G_z2 = output.mean().item()
        # Update G
        optimizerG.step()

        # Output training stats
        if i % 50 == 0:
            print('[%d/%d][%d/%d]\tLoss_D: %.4f\tLoss_G: %.4f\tD(x): %.4f\tD(G(z)): %.4f / %.4f'
                  % (epoch, num_epochs, i, len(dataloader),
                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))

        # Save Losses for plotting later
        G_losses.append(errG.item())
        D_losses.append(errD.item())

        # Check how the generator is doing by saving G's output on fixed_noise
        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):
            with torch.no_grad():
                fake = netG(fixed_noise).detach().cpu()
            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))

        iters += 1
```

After training, we can visualize the generator and discriminator losses separately:

```
plt.figure(figsize=(10,5))
plt.title("Generator and Discriminator Loss During Training")
plt.plot(G_losses,label="G")
plt.plot(D_losses,label="D")
plt.xlabel("iterations")
plt.ylabel("Loss")
plt.legend()
plt.show()
```

![DCGAN Disciminator and Generator losses]({{ '/assets/images/team13/dcgan_losses.png' | relative_url }})
{: style="width: 100%; max-width: 100%;"}
*Fig 8. DCGAN Disciminator and Generator losses*.

The loss curves are somewhat intuitive. Initially the Generator is pretty bad, so its loss is high, but the strong gradient flow quickly brings its loss down to a level similar to Discirminator's loss.

We can also display some final results from the Generator:

![Generator outputs]({{ '/assets/images/team13/dcgan_final.png' | relative_url }})
{: style="width: 100%; max-width: 100%;"}
*Fig 9. Generator outputs*.


## Basic Syntax
### Image
Please create a folder with the name of your team id under /assets/images/, put all your images into the folder and reference the images in your main content.

You can add an image to your survey like this:
![YOLO]({{ '/assets/images/UCLAdeepvision/object_detection.png' | relative_url }})
{: style="width: 400px; max-width: 100%;"}
*Fig 1. YOLO: An object detection method in computer vision* [1].

Please cite the image if it is taken from other people's work.


### Table
Here is an example for creating tables, including alignment syntax.

|             | column 1    |  column 2     |
| :---        |    :----:   |          ---: |
| row1        | Text        | Text          |
| row2        | Text        | Text          |



### Code Block
```
# This is a sample code block
import torch
print (torch.__version__)
```


### Formula
Please use latex to generate formulas, such as:

$$
\tilde{\mathbf{z}}^{(t)}_i = \frac{\alpha \tilde{\mathbf{z}}^{(t-1)}_i + (1-\alpha) \mathbf{z}_i}{1-\alpha^t}
$$

or you can write in-text formula $$y = wx + b$$.

### More Markdown Syntax
You can find more Markdown syntax at [this page](https://www.markdownguide.org/basic-syntax/).

## Reference
Please make sure to cite properly in your work, for example:

[1] Rombach, Robin, et al. "High-resolution image synthesis with latent diffusion models." Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.

[2] Radford, Alec, Luke Metz, and Soumith Chintala. "Unsupervised representation learning with deep convolutional generative adversarial networks." arXiv preprint arXiv:1511.06434 (2015).

[3] Gatys, Leon A., Alexander S. Ecker, and Matthias Bethge. "A neural algorithm of artistic style." arXiv preprint arXiv:1508.06576 (2015).

[4] Zhang, Lvmin, Anyi Rao, and Maneesh Agrawala. "Adding conditional control to text-to-image diffusion models." Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

[5] Li, D.-C.; Chen, S.-C.; Lin, Y.-S.; Huang, K.-C. A Generative Adversarial Network Structure for Learning with Small Numerical Data Sets. Appl. Sci. 2021, 11, 10823. https://doi.org/10.3390/app112210823


[1] Redmon, Joseph, et al. "You only look once: Unified, real-time object detection." *Proceedings of the IEEE conference on computer vision and pattern recognition*. 2016.

---
